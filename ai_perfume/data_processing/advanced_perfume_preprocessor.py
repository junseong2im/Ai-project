#!/usr/bin/env python3
"""
ê³ ê¸‰ í–¥ìˆ˜ ë°ì´í„° ì „ì²˜ë¦¬ ë° ë”¥ëŸ¬ë‹ í›ˆë ¨ ì‹œìŠ¤í…œ
"""

import pandas as pd
import numpy as np
import json
import re
import ast
from typing import Dict, List, Tuple, Any, Optional
from pathlib import Path
import logging
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pickle

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PerfumeDataset(Dataset):
    """í–¥ìˆ˜ ë°ì´í„°ì…‹ í´ë˜ìŠ¤"""
    
    def __init__(self, features: np.ndarray, targets: np.ndarray):
        self.features = torch.FloatTensor(features)
        self.targets = torch.FloatTensor(targets)
    
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        return self.features[idx], self.targets[idx]

class PerfumeNeuralNetwork(nn.Module):
    """í–¥ìˆ˜ ì¶”ì²œì„ ìœ„í•œ ë”¥ëŸ¬ë‹ ëª¨ë¸"""
    
    def __init__(self, input_dim: int, hidden_dims: List[int], output_dim: int, dropout_rate: float = 0.3):
        super(PerfumeNeuralNetwork, self).__init__()
        
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU(),
                nn.BatchNorm1d(hidden_dim),
                nn.Dropout(dropout_rate)
            ])
            prev_dim = hidden_dim
        
        layers.append(nn.Linear(prev_dim, output_dim))
        
        self.network = nn.Sequential(*layers)
        
    def forward(self, x):
        return self.network(x)

class AdvancedPerfumePreprocessor:
    """ê³ ê¸‰ í–¥ìˆ˜ ë°ì´í„° ì „ì²˜ë¦¬ê¸°"""
    
    def __init__(self, data_path: str):
        self.data_path = Path(data_path)
        self.df = None
        self.processed_data = {}
        
        # ì „ì²˜ë¦¬ì— ì‚¬ìš©í•  ë„êµ¬ë“¤
        self.gender_encoder = LabelEncoder()
        self.tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        self.scaler = StandardScaler()
        
        # í–¥ë£Œ ë…¸íŠ¸ ë§¤í•‘
        self.note_categories = {
            'citrus': ['citrus', 'bergamot', 'lemon', 'orange', 'grapefruit', 'lime', 'mandarin'],
            'floral': ['floral', 'rose', 'jasmine', 'lily', 'violet', 'tuberose', 'white floral', 'yellow floral'],
            'woody': ['woody', 'cedar', 'sandalwood', 'pine', 'cypress', 'guaiac wood'],
            'oriental': ['amber', 'vanilla', 'musk', 'oud', 'incense', 'benzoin'],
            'fresh': ['fresh', 'aquatic', 'green', 'herbal', 'mint', 'eucalyptus'],
            'spicy': ['warm spicy', 'fresh spicy', 'cinnamon', 'nutmeg', 'cardamom', 'black pepper'],
            'fruity': ['fruity', 'apple', 'berry', 'peach', 'plum', 'raspberry'],
            'gourmand': ['vanilla', 'chocolate', 'caramel', 'honey', 'coffee', 'almond', 'coconut'],
            'animalic': ['animalic', 'leather', 'musk', 'ambergris']
        }
        
        logger.info("í–¥ìˆ˜ ë°ì´í„° ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def load_data(self) -> pd.DataFrame:
        """ë°ì´í„° ë¡œë“œ"""
        try:
            self.df = pd.read_csv(self.data_path)
            logger.info(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.df)} ê°œì˜ í–¥ìˆ˜ ë°ì´í„°")
            return self.df
        except Exception as e:
            logger.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def clean_data(self) -> pd.DataFrame:
        """ë°ì´í„° ì •ë¦¬ ë° í´ë¦¬ë‹"""
        logger.info("ë°ì´í„° í´ë¦¬ë‹ ì‹œì‘...")
        
        # ê¸°ë³¸ í´ë¦¬ë‹
        self.df = self.df.dropna(subset=['Name', 'Description'])
        
        # í‰ì  ë°ì´í„° ì²˜ë¦¬
        self.df['Rating Value'] = pd.to_numeric(self.df['Rating Value'], errors='coerce')
        self.df['Rating Count'] = self.df['Rating Count'].astype(str).str.replace(',', '')
        self.df['Rating Count'] = pd.to_numeric(self.df['Rating Count'], errors='coerce')
        
        # ëˆ„ë½ëœ í‰ì ì„ í‰ê· ìœ¼ë¡œ ëŒ€ì²´
        self.df['Rating Value'].fillna(self.df['Rating Value'].mean(), inplace=True)
        self.df['Rating Count'].fillna(0, inplace=True)
        
        # Main Accords ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        def parse_accords(accords_str):
            try:
                if pd.isna(accords_str) or accords_str == '[]':
                    return []
                return ast.literal_eval(accords_str)
            except:
                return []
        
        self.df['Main Accords'] = self.df['Main Accords'].apply(parse_accords)
        
        logger.info(f"í´ë¦¬ë‹ ì™„ë£Œ: {len(self.df)} ê°œì˜ í–¥ìˆ˜ ë°ì´í„° ë‚¨ìŒ")
        return self.df
    
    def extract_features(self) -> Dict[str, np.ndarray]:
        """íŠ¹ì„± ì¶”ì¶œ"""
        logger.info("íŠ¹ì„± ì¶”ì¶œ ì‹œì‘...")
        
        features = {}
        
        # 1. ì„±ë³„ ì¸ì½”ë”©
        gender_mapping = {
            'for women': 0,
            'for men': 1, 
            'for women and men': 2
        }
        features['gender'] = self.df['Gender'].map(gender_mapping).fillna(2).values
        
        # 2. í‰ì  íŠ¹ì„±
        features['rating_value'] = self.df['Rating Value'].values
        features['rating_count_log'] = np.log1p(self.df['Rating Count'].values)
        
        # 3. í…ìŠ¤íŠ¸ íŠ¹ì„± (ì„¤ëª…)
        descriptions = self.df['Description'].fillna('').astype(str)
        description_tfidf = self.tfidf_vectorizer.fit_transform(descriptions).toarray()
        features['description_tfidf'] = description_tfidf
        
        # 4. í–¥ë£Œ ë…¸íŠ¸ ì¹´í…Œê³ ë¦¬ íŠ¹ì„±
        note_features = self._extract_note_features()
        features.update(note_features)
        
        # 5. ì„¤ëª… ê¸¸ì´ íŠ¹ì„±
        features['description_length'] = np.array([len(str(desc)) for desc in descriptions])
        
        # 6. ë¸Œëœë“œ íŠ¹ì„± (ì´ë¦„ì—ì„œ ë¸Œëœë“œ ì¶”ì¶œ)
        brands = self.df['Name'].str.split().str[-1]  # ë§ˆì§€ë§‰ ë‹¨ì–´ë¥¼ ë¸Œëœë“œë¡œ ê°€ì •
        brand_encoder = LabelEncoder()
        features['brand'] = brand_encoder.fit_transform(brands.fillna('Unknown'))
        
        logger.info(f"íŠ¹ì„± ì¶”ì¶œ ì™„ë£Œ: {len(features)} ê°œì˜ íŠ¹ì„± ê·¸ë£¹")
        return features
    
    def _extract_note_features(self) -> Dict[str, np.ndarray]:
        """í–¥ë£Œ ë…¸íŠ¸ ê¸°ë°˜ íŠ¹ì„± ì¶”ì¶œ"""
        note_features = {}
        
        # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ íŠ¹ì„± ìƒì„±
        for category, notes in self.note_categories.items():
            category_scores = []
            
            for _, row in self.df.iterrows():
                accords = row['Main Accords']
                if not isinstance(accords, list):
                    accords = []
                
                # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ë…¸íŠ¸ê°€ ì–¼ë§ˆë‚˜ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ê³„ì‚°
                score = sum(1 for accord in accords if any(note in accord.lower() for note in notes))
                category_scores.append(score)
            
            note_features[f'note_{category}'] = np.array(category_scores)
        
        # ì´ ë…¸íŠ¸ ìˆ˜
        note_features['total_notes'] = np.array([len(accords) if isinstance(accords, list) else 0 
                                               for accords in self.df['Main Accords']])
        
        return note_features
    
    def create_training_data(self, features: Dict[str, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:
        """í›ˆë ¨ìš© ë°ì´í„° ìƒì„±"""
        logger.info("í›ˆë ¨ ë°ì´í„° ìƒì„± ì¤‘...")
        
        # ëª¨ë“  íŠ¹ì„±ì„ ê²°í•©
        feature_arrays = []
        feature_names = []
        
        for key, values in features.items():
            if values.ndim == 1:
                feature_arrays.append(values.reshape(-1, 1))
                feature_names.append(key)
            else:
                feature_arrays.append(values)
                feature_names.extend([f"{key}_{i}" for i in range(values.shape[1])])
        
        X = np.concatenate(feature_arrays, axis=1)
        
        # íƒ€ê²Ÿ: í‰ì  ì˜ˆì¸¡ + ì¹´í…Œê³ ë¦¬ ì˜ˆì¸¡
        y_rating = features['rating_value'].reshape(-1, 1)
        
        # ì„±ë³„ ì¹´í…Œê³ ë¦¬ë¥¼ ì›-í•« ì¸ì½”ë”©
        y_gender = np.eye(3)[features['gender'].astype(int)]
        
        # íƒ€ê²Ÿ ê²°í•©
        y = np.concatenate([y_rating, y_gender], axis=1)
        
        # ì •ê·œí™”
        X = self.scaler.fit_transform(X)
        
        logger.info(f"í›ˆë ¨ ë°ì´í„° ìƒì„± ì™„ë£Œ: X shape {X.shape}, y shape {y.shape}")
        return X, y
    
    def save_preprocessed_data(self, X: np.ndarray, y: np.ndarray, 
                             output_dir: str = "data/processed") -> None:
        """ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # ë°ì´í„° ì €ì¥
        np.save(output_path / "X_features.npy", X)
        np.save(output_path / "y_targets.npy", y)
        
        # ì „ì²˜ë¦¬ ë„êµ¬ë“¤ ì €ì¥
        with open(output_path / "preprocessor_tools.pkl", "wb") as f:
            pickle.dump({
                'scaler': self.scaler,
                'tfidf_vectorizer': self.tfidf_vectorizer,
                'note_categories': self.note_categories
            }, f)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            'feature_dim': X.shape[1],
            'target_dim': y.shape[1],
            'num_samples': X.shape[0],
            'preprocessing_info': {
                'tfidf_features': self.tfidf_vectorizer.get_feature_names_out().tolist() if hasattr(self.tfidf_vectorizer, 'get_feature_names_out') else [],
                'note_categories': list(self.note_categories.keys())
            }
        }
        
        with open(output_path / "metadata.json", "w", encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_path}")
    
    def train_deep_learning_model(self, X: np.ndarray, y: np.ndarray, 
                                model_save_path: str = "models/perfume_dl_model.pth") -> Dict[str, Any]:
        """ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨"""
        logger.info("ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
        
        # ë°ì´í„° ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # ë°ì´í„°ì…‹ ìƒì„±
        train_dataset = PerfumeDataset(X_train, y_train)
        test_dataset = PerfumeDataset(X_test, y_test)
        
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
        
        # ëª¨ë¸ ìƒì„±
        input_dim = X.shape[1]
        output_dim = y.shape[1]
        hidden_dims = [256, 128, 64]
        
        model = PerfumeNeuralNetwork(input_dim, hidden_dims, output_dim)
        
        # ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì €
        criterion_rating = nn.MSELoss()
        criterion_category = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10)
        
        # í›ˆë ¨
        num_epochs = 100
        best_loss = float('inf')
        training_history = {'train_loss': [], 'test_loss': []}
        
        for epoch in range(num_epochs):
            model.train()
            train_loss = 0
            
            for batch_x, batch_y in train_loader:
                optimizer.zero_grad()
                
                outputs = model(batch_x)
                
                # í‰ì  ì˜ˆì¸¡ ì†ì‹¤
                rating_loss = criterion_rating(outputs[:, 0:1], batch_y[:, 0:1])
                
                # ì¹´í…Œê³ ë¦¬ ì˜ˆì¸¡ ì†ì‹¤ (ì„±ë³„)
                category_loss = criterion_category(outputs[:, 1:4], 
                                                 torch.argmax(batch_y[:, 1:4], dim=1))
                
                # ì´ ì†ì‹¤
                total_loss = rating_loss + 0.5 * category_loss
                
                total_loss.backward()
                optimizer.step()
                
                train_loss += total_loss.item()
            
            # ê²€ì¦
            model.eval()
            test_loss = 0
            with torch.no_grad():
                for batch_x, batch_y in test_loader:
                    outputs = model(batch_x)
                    rating_loss = criterion_rating(outputs[:, 0:1], batch_y[:, 0:1])
                    category_loss = criterion_category(outputs[:, 1:4], 
                                                     torch.argmax(batch_y[:, 1:4], dim=1))
                    total_loss = rating_loss + 0.5 * category_loss
                    test_loss += total_loss.item()
            
            avg_train_loss = train_loss / len(train_loader)
            avg_test_loss = test_loss / len(test_loader)
            
            training_history['train_loss'].append(avg_train_loss)
            training_history['test_loss'].append(avg_test_loss)
            
            scheduler.step(avg_test_loss)
            
            if avg_test_loss < best_loss:
                best_loss = avg_test_loss
                torch.save({
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'epoch': epoch,
                    'loss': best_loss,
                    'model_config': {
                        'input_dim': input_dim,
                        'hidden_dims': hidden_dims,
                        'output_dim': output_dim
                    }
                }, model_save_path)
            
            if (epoch + 1) % 10 == 0:
                logger.info(f'Epoch [{epoch+1}/{num_epochs}], '
                          f'Train Loss: {avg_train_loss:.4f}, '
                          f'Test Loss: {avg_test_loss:.4f}')
        
        logger.info(f"ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ. ìµœê³  ì„±ëŠ¥: {best_loss:.4f}")
        
        return {
            'best_loss': best_loss,
            'training_history': training_history,
            'model_path': model_save_path,
            'test_performance': self._evaluate_model(model, test_loader)
        }
    
    def _evaluate_model(self, model: nn.Module, test_loader: DataLoader) -> Dict[str, float]:
        """ëª¨ë¸ í‰ê°€"""
        model.eval()
        rating_errors = []
        category_correct = 0
        total_samples = 0
        
        with torch.no_grad():
            for batch_x, batch_y in test_loader:
                outputs = model(batch_x)
                
                # í‰ì  ì˜ˆì¸¡ ì˜¤ì°¨
                rating_pred = outputs[:, 0:1]
                rating_true = batch_y[:, 0:1]
                rating_errors.extend(torch.abs(rating_pred - rating_true).cpu().numpy())
                
                # ì¹´í…Œê³ ë¦¬ ì •í™•ë„
                category_pred = torch.argmax(outputs[:, 1:4], dim=1)
                category_true = torch.argmax(batch_y[:, 1:4], dim=1)
                category_correct += (category_pred == category_true).sum().item()
                total_samples += batch_y.shape[0]
        
        return {
            'rating_mae': np.mean(rating_errors),
            'rating_rmse': np.sqrt(np.mean(np.square(rating_errors))),
            'category_accuracy': category_correct / total_samples
        }

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        # ë°ì´í„° ê²½ë¡œ ì„¤ì •
        data_path = "C:/Users/user/Desktop/ai project/ai_perfume/data/raw/raw_perfume_data.csv"
        
        # ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        preprocessor = AdvancedPerfumePreprocessor(data_path)
        
        # ë°ì´í„° ë¡œë“œ ë° í´ë¦¬ë‹
        preprocessor.load_data()
        preprocessor.clean_data()
        
        # íŠ¹ì„± ì¶”ì¶œ
        features = preprocessor.extract_features()
        
        # í›ˆë ¨ ë°ì´í„° ìƒì„±
        X, y = preprocessor.create_training_data(features)
        
        # ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
        preprocessor.save_preprocessed_data(X, y)
        
        # ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨
        model_save_path = "C:/Users/user/Desktop/ai project/ai_perfume/models/perfume_dl_model.pth"
        Path(model_save_path).parent.mkdir(parents=True, exist_ok=True)
        
        training_results = preprocessor.train_deep_learning_model(X, y, model_save_path)
        
        # ê²°ê³¼ ì¶œë ¥
        logger.info("="*80)
        logger.info("ğŸ‰ ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
        logger.info("="*80)
        logger.info(f"ğŸ“Š ìµœì¢… ì†ì‹¤ê°’: {training_results['best_loss']:.4f}")
        logger.info(f"ğŸ“ˆ í‰ì  ì˜ˆì¸¡ MAE: {training_results['test_performance']['rating_mae']:.4f}")
        logger.info(f"ğŸ“ˆ í‰ì  ì˜ˆì¸¡ RMSE: {training_results['test_performance']['rating_rmse']:.4f}")
        logger.info(f"ğŸ¯ ì¹´í…Œê³ ë¦¬ ì •í™•ë„: {training_results['test_performance']['category_accuracy']:.3f}")
        logger.info(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {training_results['model_path']}")
        
        # í›ˆë ¨ ê¸°ë¡ ì €ì¥
        with open("C:/Users/user/Desktop/ai project/ai_perfume/models/training_results.json", "w") as f:
            json.dump({
                'best_loss': training_results['best_loss'],
                'test_performance': training_results['test_performance'],
                'model_path': training_results['model_path']
            }, f, indent=2)
        
        logger.info("âœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except Exception as e:
        logger.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()